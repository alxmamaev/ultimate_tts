feature_extractor_params:
  sample_rate: 22050
  wav_max_value: 32768.0
  n_fft: 1024
  n_mels: &n_mels 80
  f_min: 80
  f_max: 8000
  win_length: 1024
  hop_length: 256
  power: 1.0
  trim_front_params: # Check torchaudio VAD doc
    trigger_level: 7.0
    trigger_time: 0.25
  # trim_back_params: # Check torchaudio VAD doc
  #   trigger_level: 7.0
  #   trigger_time: 0.25

training_params:
  vocab: &vocab " .,!?abcdefghijklmnopqrstuvwxyz"
  num_epochs: &num_epochs 400
  batch_size: &batch_size 2
  num_workers: &num_workers 0
  
  loss_fn: &loss_fn Tacotron2Loss
  optimizer_fn: &optimizer_fn Adam
  lr: &lr 0.001
  grad_clip_value: &grad_clip_value 1.0

data_params:
  train_metadata: "./data/train_metadata.csv"
  valid_metadata: "./data/valid_metadata.csv"
  mels_datapath: "./data/mels"
  wavs_datapath: "./data/wavs" # Not using in tacotron training
  collate_fn: text_mel_collate_fn
  tokenizer: 
    _target_: CharTokenizer
    vocab: *vocab
  cleaners:
    - _target_: LowerCaseCleaner
    - _target_: VocabCleaner
      vocab: *vocab

model:
  _target_: Tacotron2
  vocab_size: 32
  n_mels: *n_mels
  vocab_embedding_size: 512
  encoder_embedding_size: 512
  encoder_n_convolutions: 3
  encoder_kernel_size: 5
  prenet_layer_size: 256
  decoder_embedding_size: 512
  attention_embedding_size: 128
  attention_location_n_filters: 32
  attention_location_kernel_size: 31
  postnet_n_convolutions: 5
  postnet_kernel_size: 5

inference:
  output_key: ["mels", "alignments", "durations"]
  mel_key: "mels"

args:
  expdir: "ultimate_tts"
  logdir: &logdir "./logs/"
  seed: 42
  distributed: False
  apex: False
  amp: False
  verbose: False
  timeit: False
  check: False
  overfit: False
  deterministic: True
  benchmark: False

runner:
  _target_: TTSRunner
  input_key: &model_input ["texts", "encoder_mask", "mels_target"]
  output_key: &model_output ["decoder_outputs", "postnet_outputs", "alignments", "gate_outputs"]
  target_key: &model_target ["mels_target", "decoder_mask", "gates_target"]
  loss_key: &model_loss "loss"

engine:
  _target_: DeviceEngine

loggers:
  console:
    _target_: ConsoleLogger
  tensorboard:
    _target_: TensorboardLogger
    logdir: *logdir

stages:
  train:
    num_epochs: *num_epochs

    loaders:
      batch_size: *batch_size
      num_workers: *num_workers

    criterion:
      _target_: *loss_fn

    optimizer:
      _target_: *optimizer_fn
      lr: *lr

    callbacks:
      loss:
        _target_: CriterionCallback
        input_key: *model_output
        target_key: *model_target
        metric_key: *model_loss
      optimizer:
        _target_: OptimizerCallback
        metric_key: *model_loss
        grad_clip_fn: clip_grad_value_
        grad_clip_params:
          clip_value: *grad_clip_value
      alignments:
        _target_: TTSOutputsLogger
        outputs_keys: ["decoder_outputs", "postnet_outputs", "alignments"]
      saver:
        _target_: CheckpointCallback
        logdir: *logdir
        loader_key: valid
        metric_key: loss
        minimize: True
        use_logdir_postfix: True
      verbose:
        _target_: TqdmCallback